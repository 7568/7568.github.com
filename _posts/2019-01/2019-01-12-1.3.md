#### 1.3 条件概率与独立性

​	目前为止，我们所讨论的概率都是无条件的概率，即，给定了一个样本空间，所有的概率都根据这个样本空间计算得到，然而在许多情况下，我们需要根据新获取的信息跟新样本空间。此时，计算概率的方法也有所不同，我们称之为计算条件概率（conditional probabilities）.

​	**例1.3.1 （四张A）**从一副洗匀的扑克牌中自上而下发四张牌，发到四张A的概率是多少？利用上一节给出的犯法，我们可以求出这个概率。四张牌的不同组合数为：

​				                             	$\left(\begin{array}{c} 4 \\ 52 \end{array}\right)$ = $$270 \  725$$ 

其中只有一种组合是四张A，并且所有组合等概率，于是发送到四张A的概率是$1/270\ 725$ 。

​	我们还可以按照下面“不断更新”的思路来计算这个概率：第一张牌是A的概率为$4/52$，假定第1张是A，那么第2张牌是A的概率为$3/52$（余下51张牌中有3张A）。依次下去，所求概率为：

​			                 	$\displaystyle\frac{4}{52}\times\frac{3}{51}\times\frac{2}{50}\times\frac{1}{49} = \frac{1}{270\ 725}$                                                                             $\Iota\Iota$

​	在上题的第二种解法中，每法一张牌，我们相应更改样本空间，实际计算的是事件的条件概率。

**定义1.3.2** 设A，B为S中的事件，且P(B)>0，**则在事件B发生的条件下事件A发生的条件概率（conditional probability of A given B）**记作P（A|B），表示为

（1.3.1）                                                $P(A|B) =  \displaystyle\frac{P(A\cap B)}{P(B)}$

​	注意，计算条件概率时，样本空间是B：P(B|B) = 1，直观上，初始的样本空间S变成了B；所有后续事件发生的概率也都根据他们与B之间的关系有所调整。特别的，可以考虑两个不相交事件的田间概率，设A，B不交，则$P(A\cap B) = 0$ , 于是$P(A|B) = P(B|A) = 0$.

**例1.3.3（例1.3.1-续）** 尽管发到全部四张A的概率非常小，我们还是来看看这一事件在“已发生若干张A”下的条件概率是如何变化的。仍从一副洗匀的扑克牌中发四张牌，现在我们计算

​					P(4张牌里有4张A | 前$i$张牌里有$i$张A), $i=1,2,3$

事件｛4张牌里有4张A｝显然是｛前$i$张牌里有$i$张A｝的子集，于是，根据条件概率的定义（1.3.1），我们有

​		P(4张牌里有4张A|前$i$张牌里有$i$张A) = $\displaystyle\frac{P({4张牌里有4张A}\bigcap {前i张牌里有i张A})}{P({前i张牌里有i张A})}$ 

​									      = $\displaystyle\frac{P({4张牌里有4张A})}{P({前i张牌里有i张A})}$

其中分子已经求得，分母也可类似推导出来。前$i$张牌的不同组合总数为$\left(\begin{array}{c} 52\\ i \end{array}\right)$，故

​						P(前$i$张牌里有$i$张A) = $\displaystyle\frac{\left(\begin{array}{c} i\\4 \end{array}\right)}{\left(\begin{array}{c} 52 \\ i \end{array}\right)}$

于是，条件概率为

P(4张牌里有4张A|前$i$张牌里有$i$张A) = $\displaystyle\frac{\left(\begin{array}{c} 52\\i \end{array}\right)}{\left(\begin{array}{c} 52\\4 \end{array}\right)\left(\begin{array}{c} i\\4 \end{array}\right)}$ = $\displaystyle\frac{(4-i)!\ 48!}{(52-i)!}$ = $\displaystyle\frac{1}{\left(\begin{array}{c} 52-i\\4-i\end{array}\right)}$

当$i=1,2,3$时，其值分别为0.00005 ， 0.00082和0.02041 .                                                                    $\Iota\Iota$

对于任意满足P（B）>0的事件B，可以直接验证概率函数P($\cdot$|B)满足Kolmogorov公理（见习题1.35）。也许你会怀疑“P(B)>0”的条件是否多余$——$有谁会以零概率事件为前提条件？然而有趣的是，这一思路往往能够另辟蹊径地巧妙地解决一些问题，我们将在第四章加一讨论。

​	条件概率有时候难以理解，需要我们认真分析和思考，下面的例讲述的是一个广为人知的故事。

​	**例 1.3.4 （三个囚徒）**监狱的死囚牢里关押了A，B，C三个囚犯，狱长决定赦免三个人中的一人，并且随机的选择了一人作为被赦的人，他将自己的选择告诉了监狱看守，但是要求看守几天之内不能泄漏被赦免人的姓名。

第二天，A问看守谁被赦免了，看守拒绝回答，A接着又问B和C两人中哪个人会被处死，看守想了一会儿，告诉他B将被处死。

​	**看守的推理：**每名囚犯被赦免的概率都是$\frac{1}{3}$.显然，B和C当中必然有一个要被处死，所以我并没有向A透露有关A是否被赦免的任何消息。

​	**A的推断：**假定B将被处死，那么A和C当中必有一人被赦免。即，我被赦免的概率增加到了$\frac{1}{2}$。

​	两人当中，看守的推理是正确的，让我们来看看为什么。分别以A，B和C表示A，B或者C被赦免这三件事，我们有P(A = P(B) = P(C) = $\frac{1}{3}$。以W表示看守说B将被处死这一件事。根据式（1.3.1），A可推知自己被赦免的概率满足：

​					P(A|W) = $\displaystyle\frac{P(A\cap W)}{P(W)}$

看守的推理可以概括为下表：

| 被赦免的囚犯 |                         看守告诉A                         |
| :----------: | :-------------------------------------------------------: |
|   A<br />A   | $\begin{cases}B将被处死 \\ C将会被处死\end{cases}$ 等概率 |
|      B       |                         C将被处死                         |
|      C       |                         B将被处死                         |

根据此表，可算的

​				      P(W) = P(看守说B将被处死) 

​				        	= P(看守说B将被处死且A被赦免) 

​				        	 $+$ P(看守说B将被处死且C被赦免) + P(看守说B将被处死且B被赦免)

​				        	= $\displaystyle\frac{1}{6} + \frac{1}{3} + 0 = \frac{1}{2}$

 因此，根据看守的推理，我们有

​				$\displaystyle P(A|W) = \frac{A\cap W}{P(W)}$ = $\displaystyle\frac{P(看守说B将被处死，且A被赦免)}{P(看守说B将被处死)}$ = $\displaystyle\frac{1/6}{1/2}$ = $\displaystyle\frac{1}{3}$

​	从本例可以看出，条件概率有时很难把握，需要细心理解。本例所衍生的其他问题，可以参阅1.37。

​	式（1.3.1）整理后可以得到事件交的概率的计算公式：

（1.3.3）                          $\displaystyle P(A\cap B) = P(A|B)P(B)$，

它实际上就是例1.3.1中所用的公式，利用式（1.3.3）的对称性，我们还有

（1.3.4）                         $P(A\cap B) = P(B|A)P(A)$

在处理复杂计算时，可以根据需要选用式（1.3.3）或式（1.3.4）进行化简，此处，注意到两式右端相等，（整理后）即的颠倒次序的两个条件概率之间的关系式

（1.3.5）                        $P(A|B) = P(B|A)\displaystyle\frac{P(A)}{P(B)}$

式（1.3.5）通常以其发现者Thomas Bayes（又见Stigler 1983）命名，称作Bayes公式。

将Bayes公式应用于样本空间的划分，得到一个比式（1.3.5）更一般的形式，我们将它作为Bayes公式的定义，如下面定理所述：

**定理1.3.5 （Bayes公式）**设$$A_1，A_2 …$$ 为样本空间的划分， $B$为任意集合。则对$i$=1,2,…,有

​                                                        $$P(A|B) = \displaystyle\frac{P(B|A_i)P(A_i)}{\displaystyle\sum_{j=1}^\infty P(B|A_j)P(A_j)}$$

**例1.3.6（编码）** 消息被编码后发送出去，信号在传输过程中可能会出错，Morse码才用点，划两种信号，两者出现的比例为$3:4$ 。因此，对于任意给定的信号，有

​                                          P（发送的是点信号）=$\displaystyle\frac{3}{7}$  , P（发送的是划信号）=$\displaystyle\frac{4}{7}$ 

假设传输线路上有干扰，它将点信号误传为划信号的概率为$\displaystyle\frac{1}{8}$ ，将划信号误传为点信号的概率亦相同。那么如果收到一个点信号，是否能确定发送的就是点信号？根据Bayes公式，我们有

 P（发送的是点信号 | 收到的是点信号） =  P（收到的是点信号|发送的是点信号） $\displaystyle\frac{P（发送的是点信号）}{P（收到的是点信号）}$

由已知，有P（发送的是点信号）=$\displaystyle\frac {3}{7}$ 以及P（收到的是点信号|发送的是点信号）=$\displaystyle \frac{7}{8}$ .此外

P（收到的是点信号） = P（收到的是点信号$\bigcup$发送的是点信号）+P（收到的是点信号$\bigcap$发送的是划信号）

​                                   = P（收到的是点信号|发送的是点信号）P（发送的是点信号）

​                                   $+$ P（收到的是点信号|发送的是划信号）P（发送的是划信号）

​                                   = $\displaystyle\frac{7}{8} \times \frac{3}{7} - \frac{1}{8}\times\frac{4}{7}=\frac{25}{56}​$

综合上述结果，可知正确收到一个点信号的概率是 

​                                P（发送的是点信号|收到是点信号）=$\displaystyle \frac{(\frac{7}{8})\times(\frac{3}{7})}{\frac{25}{56}}$=$\displaystyle\frac{21}{25}$

​	在某些情况下，事件$B$出现的概率可能不影响事件$A$的概率，用符号表示即

（1.3.6）                                              P(A|B) = P(A)

若上式成立，则根据Bayes公式（1.3.5）以及公式（1.3.6）有

（1.3.7）        P（A|B） = P(A|B) $\displaystyle\frac{P(B)}{P(A)}$ = P(A) $\displaystyle\frac{P(B)}{P(A)}$=P(B)

因此事件$A$发生对事件$B$也没什么影响。此外，由于P(B|A)P(A) = P(A$\cap$B)，还有

​                        P（A$\cup$B） = $P(A)P(B)$

我们将上式作为统计独立性的定义。

***定义1.3.7 称事件A，B统计独立（statistically  independent）***，如果

（1.3.8）             $P$(A$\cap$B） =$ P(A)P(B)$

   注意，统计独立性也可以等价德用式（1.3.6）或者（1.3.7）来定义（只需 $P(A) > 0$ 或者 $P(B) > 0$）.采用式（1.3.8）的好处在于该式中事件$A$，$B$，是对称的，因而该式更容易推广到两个以上的时间。

​	许多赌博游戏都有独立事件的模型，例如，练习转轮盘和练习掷双骰都是独立事件。

​	**例1.3.8（Mere掷骰问题）**本章开头提到Chevalier de Mere在赌博问题中特别关注事件“掷骰四次至少有一次是6点”的概率，事实上，

​		P（掷骰四次至少有一次是6点）=1 - $P$(掷骰四次都没有6点)

​								       = 1 - $\displaystyle\prod_{i-1}^{4}$$P$(第$i$次掷骰的结果不是6点)

其中，最后一个等式成立的依据是连续掷骰的独立性。每次投掷，结果不是6点的概率为$\displaystyle\frac{5}{6}$，于是

​			$P（投掷四次至少有一次是6点） = 1 - (\frac{5}{6})^4 = 0.518$

事件A，B的独立性表明他们的补也是独立的。事实上我们有下述定理：

定理1.3.9 设A，B为相互独立的事件，则下列几对事件都是独立的：

a. A和$B^C$ ,                          

b. $A^C$和B ,

c. $A^C$和$B^C$.

证明 这里只证明a，其余留作业题1.40。为证a，我们只需证明P(A$\cap$$B^C$) = P(A)P($B^C$)。根据定理1.2.9a，我们有

​				P(A$\cap$$B^C$) = P(A) - P(A$\cap$B)

​						= P(A) - P(A)P(B)     (由于A和B独立)

​						=P(A)(1 - P(B))

​						= P(A)P($B^C$)

两个以上事件的独立性可以仿照式（1.3.8）定义，不过此时应避免一些常见的错误，比如，你也许认为A，B和C独立条件是P(A$\cap$B$\cap$C) = P(A)P(B)P(C)，但事实并非如此。

例1.3.10（掷双骰）考虑掷双骰的试验，其样本空间为

​	S = {(1,1),(1,2), … (1,6), … (2,6), … ,(6,1), … ,(6.6)},

即样本空间有数字1，6组成的全部36个有序对所构成，指定下列事件：

​	A={两骰子点数相同} = {(1,1),(2,2),(3,3),(4,4),(5,5),(6,6)},

​	B = {两骰子点数介于7到10之间}，

​	C = {两骰子点数只和等于2,7或者8}，

通过计算以上事件包含全体36个结果，我们求得他们的概率分别为

​	P(A) = $\frac{1}{6}$ , P(B) = $\frac{1}{2}$ 以及P(C) = $\frac{1}{3}$

且

​	P(A$\cap$B$\cap$C) = P(两骰子点数只和为8，并且是两个4点) = $\frac{1}{36}$ = $\frac{1}{6}\times\frac{1}{2}\times\frac{1}{3}$=P(A)P(B)P(C)

但

​	P(B$\cap$C) = P(两骰子点数只和等于7或者8)  =$$\frac{11}{36}$$$\neq$P(B)P(C)。因此，条件“P(A$\cap$B$\cap$C) = P(A)P(B)P(C)”并不足以保证A，B和C两两独立。

​	看完上面的例字，或许你会对如何推广独立性定义萌生另一个猜想：定义A，B和C独立，当且仅当他们两两独立，不过，这种推广任然不对。

**例1.3.11（字母）**令样本空间S包括字母a，b，c 的 3！个置换以及同一个字母构成的三元组，即

​		                                    S = $\begin{Bmatrix} aaa  &bbb&ccc\\abc & bca & cba \\ acb & bac & cab\end{Bmatrix} $,

其中每个元素的概率都为$\frac{1}{9}$。令

​					$A_i$ = {三元组的第$i$个位置上是a}

容易计算

​					P($A_i$) = $\frac{1}{3}$,$i$ = 1,2,3

且

​				P($A_1\cap A_2$) = P($A_1\cap A3$) = P($A_2\cap A_3$) = $\frac{1}{9}$,

因此事件$A_i(i=1,2,3)$两两独立。然而

​			P($A_1\cap A_2\cap A_3$) $=$ $\frac{1}{9}$ $\neq$ P($A_1$)P($A_2$)P($A_3$),

故$A_i(i=1,2,3)$不满足条件“P(A$\cap B\cap C$) = P(A)P(B)P(C)”.

​	前面的两个例子表明一列事件相互独立的定义条件极强，其正确定义如下：

​	定义1.3.12 称一列事件$A_1, … ,A_n$ 相互独立（mutually independent），如果对任意$A_{i_1} , … , A_{i_k}$，都有

​		                                                P($\displaystyle\bigcap_{j=1}^{k} A_{i_j}$) = $\displaystyle\prod_{j=i}^{k}P(A_{i_j})$ 

例 1。3.13（连续三次硬币 - $\Iota$ ）考虑连掷三次硬币的试验。试验的每个样本点显然必须指明每一次掷得的结果，例如，HHT表明结果中有两次是正面朝上的有一次是反面朝上。该试验的样本空间由8个样本点构成，即

​	         S = {HHH，HHT，HTH，THH，TTH，THT，HTT，,TTT}，

令$H_i(i=1,2,3)$表示“第$i$次投掷结果为正面” 的事件，例如，

（1.3.9）          $H_1$ = {HHH，HHT，HTH，HTT}

如果规定每个样本点的概率都为$\frac{1}{8}$，则通过枚举$Hi$所含的样本点（如式（1.3.9），有$P(H_1) =P(H_2) = P(H_3) = \frac{1}{2}$ ）。这就表明：试验多有的硬币是一枚公平的硬币，每一次掷得正面朝上与掷得反面得概率相等。

​	基于这个概率模型，我们还可以证明事件$H_1 , H_2 $和$ H_3$相互独立，即每一投硬币的结果都与其每次掷得的结果无关。

​	在上面所有的概率模型中，我们为每个样本点都赋以概率$\frac{1}{8}$，可以证明，仅在这一概率模型下，才有$P(H_1) = P(H_2) = P(H_3) = \frac{1}{2}$且$H_1 , H_2 $和$H_3$相互独立。
